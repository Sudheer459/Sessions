26th June Session 55

1. Build the image
2. Run the image

Problems(in docker)
---------------
1 pg --- 10 rooms

If power cut, water cut

10PGs -- 1pg water cut
He will shift persons to other PG temporary..


1 Host --- 3 containers if host crash..


Scalability
----------------
Normal servers with route53
Traffic increases, multiple servers and keep load balanccers and update R53 as LB URL.
If container crash, docker is not able to replace automatically. --> not reliable --> self healing
No proper volume management.
NO strong network connectivity.
No auto scaling 
No secrets and config management

3 Hosts -- 1 pod is  host-1 another pod is in host-2


////////////Benefits of using kubernetes and docker///
Scalability, High avaibility, portability, security, ease of use, reduce costs, improve agility, increase innovation
------------------------------------

Docker Architecture?
client = docker command

docker daemon = docker engine.... systemctl start docker..

daemon/server responds to client

docker run nginx --> checks image available in local r not, if not available pull it and keep it in local.
Run a container and send output to client.

Docker Architecture::How your system is getting request, how is it processing inside and 
how you r sending request back to the client.


Docker Architecture consists of docker client which is docker command line tool and
docker engine is nothing but docker host and the local registry and central registry.
Whenever you run a command in docker host it will connect to the daemon, it will check for the image local 
if it is not available it will pull from the central repository and keep it in local create a container of it and send 
a response to the client.

Docker = suit = image build + image pull through its hub + running the image as containers.
Docker is providing run time environmnent to run docker images.

Kubernates is a container orchestrator....

If you want to run 100 pg, we will take help of manager.. he needs some software.. 


Amazon master under this we have ec2s(nodes).
Request come to master.
Master will decide where to go

Nodes should run containers.
containers run time environment.

Here VS code will develop docker files and push to github.

kubectl - to connect kubernetes

docker run we have to give inputs --- image, ports, versions, volumes, name, environment variables.



eksctl --> create and manage EKS cluster
kubectl --> to manage containers in kubernetes cluster


kubectl get nodes --> display nodes in k8 cluster

command eksctl create cluster --config-file=<filename>

K8 resources
---------
Namespace --> Isolated project where you can create resources related to your project.

Basic syntax

kind:
apiVersion:
metadata:

spec:


kubectl apply -f <filename.yaml>
kubectl delete -f <filename.yaml>

pod is smallest deployable(runnable) unit in kubernetes



pod(EKS) vs container(Docker)
------------
Pod contains multiple containers. Container inside pod share same n/w and storage.

-----------------

27th June Session 56

1.Image Building
	Develop in VS Code
	Push to github
	pull in workstation
	build images
	push to docker hub
	
2. Login to workstation	
	Install docker, kubectl and eksctl
	Either aws configure or attach the role
	eks.yaml
	Now create cluster through eksctl command
	kubectl get nodes

3. Develop K8 YAML files
	Push to github
	Pull in workstation
	Apply using kubectl command
	
	
labels vs annotations

1 cpu = 1000m or 1024m


resources:
	  # soft limits
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m
	
If there is a code change, should i go for build r not? 
Ans: yes build

Code and configuration

What is configuration?
DB urls, DB usernames, passwords, api endpoint, other system urls.

Keep the key inside code, but value should be out of the code.

If changes, change the value and restart the application.

How can you access pod outside?
Ans: Services

Pods are ephemeral, IP address is temporary, but you can attach this to the service.

Service: 
	Acts as here loadbalancer
	service mesh
	
Cluster IP < Node Port < Load Balancer

1.Cluster IP
2. Node Port
3. Load Balancer

Labels are acted as selectors
Internet::: Other than kubernetes cluster everthing is called internet..

Nodeport --->
Internet--- external
Intranet --- internal(inside)
-------------------
28th June Session 57

Sets
---------
1. Replicaset -- Create replicas of pod.
2. Deployment
3. Daemonset
4. Statefulset


Pod is a subset of replica.

Pod = replicaset-name-<some-random-string>

Deployment: Removing old code, releasing new version of code, changes the application version.

Stop the server
remove old code
Download new code
Restart the server.

Deployment
--------------
Replicaset = deployment-name-<random-string>

pod = RS-<random-string>


nginx-68fcf7b7bf-b9qmf

nginx-8494fb8c88-5cbgf 



Deployment >> Replicaset >> Pod >> containers


https://github.com/ahmetb/kubectx

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


For security reasons we cant claim port no 80 for nginx, we can claim port no 8080

////For best practice if we seperate configuration code from application code we can reduce steps(new builds, new deploys)////
I dont want to rebuild the image bcz configuration is not part of the code and not best practice also

We can keep nginx configuration in a configmap
---------------------------
1st July Session 58

Volumes in Kubernates
------------------
1. External HD
	SSD, Pendrive, etc.,
	
2. Google/microsoft drive

EBS/EFS
-------------
1. HD should be as near as possible to the computer.
2. Google drive, can be anywhere in the world. Network is used for file sharing. NFS(network file sharing)

Static and Dynamic provisioning

Static
------------
EBS

Mumbai-- Region

North Mumbai --AZ-1a
South Mumbai --AZ-1b

1. I need to create EBS volume
2. You need to install drivers. I have to install EBS related drivers in EKS cluster.
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
3. Your nodes should have access to connect with EBS volumes. Attach EBSCSI policy to the EC2 instance role.

wrappers..

You can represent EBS volume inside kubernetes with a resource called Persistant Volume. This is equivalent to EBS storage.


PVC
----
Persistant volume claim. Pods should claim i want the volume. Pods can claim through resource called PVC.

1. If you PV is not available
---------
check drivers installed r not
check your EC2 have tje role and permissions

If instances are created in 1A, 1B, 1C and Volume is in 1C. in this case we have to use node selectors.

nodeSelector:
    topology.kubernetes.io/zone: us-east-1c
	
and this is called as static provisioning

1. Admins
2. Users

As a Application devops engineer, i will get access to expense namespace

1. Users send email to storage team for creation of volume. AWS Storage team will create volume and give you ID.
2. Users send email to K8s admin to create PV for them the ID.
3. Users can claim using PVC.

Dynamic Provisioning
-------------
1. You need to install drivers. I have to install EBS related drivers in EKS cluster.
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
2. Your nodes should have access to connect with EBS volumes. Attach EBSCSI policy to the EC2 instance role.
3. Someone on behalf of you should create volume and equivalent PV automatically. THis is called Dynamic provisioning that
one is storage class


So admins create storage class for every project as best practice.

Static---------
Kid -- Mother -- Fater --- MOney
Pod -- PVC -- PV --- Storage/Volume


Dynamic
------------
Kid -- Mother -- E-wallet -- Money
Pod -- PVVC -- Storage class -- Storage


EFS -- 
--------
EBS vs EFS

1. EBS is like hard disk, volume and instance should be in same AZ-1a
2. EFS is through NFS protocol, it can be anywhere
3. EBS volume is static, it can increase on its own.
4. EFS is completely dynamic in storage, it can automatically expand memory.
5. Which is suitable for DB applications like MYSQL
--------------------------------------
3th July Session 58

Cluster creation with eksctl
namespaces
Pods (multicontainer, environment,labels,resources,annotations)
configmap
secrets
Services
	ClusterIP
	nodeport
	Loadbalancer
Sets
	Replicaset
	Deployment
expense using deployment
	How to use configmap as volume
Staic provisioning
	EBS
Dynamic Provisioning
	EBS
EBS vs EFS

EBS
----------
1. Create Volume
2. Install EBSCSI drivers
3. EC2 should have role to access EBS
4. Create PV equivalent to EBS Volume --> admin, CLuster level
5. Create PVC --> namespace level user can do this
6. Use PVC in pod


Dynamic
------------
SC --> admin level, this creates volume and PV automatically.
PVC, mention SC name


EFS
------------
1. Create EFS volume
2. Allow EC2 instances on NFS protocol in EFS SG
3. install EFS driveresx`

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.0" > private-ecr-driver.yaml

Apply the manifest..
kubectl apply -f private-ecr-driver.yaml


4. Check IAM access to the instances
AmazonEFSCSIDriverPolicy


LB --> Node on particular port--> Nodes should allow traffic from LB --> nodeport --> cluster IP --> pod

EFS Dynamic
---------
1. Installing drivers

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.0" > public-ecr-driver.yaml

Apply the manifest..
kubectl apply -f private-ecr-driver.yaml

2. Check EC2 instance role have access or not
3. Check EFS Volume.
4. Check EFS SG allowing EC2 SG
5. Create SC

MySQL
-----------
EBS

1. Install EBS CSI drivers
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
2. Check ebs csi permissions to node
AmazonEBSCSIDriverPolicy

3. create expense storage class, namespace

Inform application team that SC and namespace created. Drivers installed permissions given to nodes..

StatefulSet -->>> Used for stateful applications i.e., which should have some storage.
Deployment --> Used for stateless applications i.e., usually frontend and backend

Pods are created with -0, -1 in statefulset, because statefulset should keep the pod identity.
Pods in deployment are created at a time, but pods in statefulset will be created in orderly manner.

Stateful set must have headless service. 

A service which will not have cluster IP is called Headless

While dealing database cluster, one node should be aware of other nodes.

-------------------------------
4th July Session 60

PV -- Representing physical storage
PVC --Claiming the storage
SC -- Automatic creation of volume and PV based on the claim from PVC

Deployment vs Statefulset
--------------------
1. Stateful application like DB
2. PV and PVC are mandatory for statefulset.
3. Orderly provisioning of pods happens.
4. Pods keep their identity like name
5. We must create headless service.
6. Every pod should have its own storage. So PV and PVC


1. Install EBS drivers
2. Check EC2 role permissions
3. Create storage class
4. Create statefulset

'''''''''''''''''''' Statefulset
If deployment, nslookup of service gives, service IP address
If statefulset,nslookup of headless service gives us IP address of all pods.
''''''''''''''''''''
Headless service is for Cluster features
Normal service is for to connect external users.


HELM
------------
1. Building image
2. Running image

Only few values frequently change in manifest files.

1. Kubernetes package manager
2. templatise manifest files

HELM install commands...
$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh
......

1. Chart.yaml

Install helm chart

helm install <preferred-name>. --> . represents Chart.yaml in current directory

helm uninstall nginx
help history nginx
helm rollback nginx
helm list
helm upgrade nginx
helm status nginx

-----------------------
5th July Session 61

RBAC(Role Based access control)
--------------
expense

trainee - read access
Sr engineer -- deployments access
TL -- namespace admin access
Manager -- cluster admin(all access)

Authentication and Authorization


Role
RoleBinding -- user ni roleki bind cheyadam
ClusterRole
ClusterRoleBinding

AWS IAM -- RBAC for AWS IAM
EKS --PaaS, AWS EKS can use IAM as authentication provider. SSO

1. TL will send an email to create namespace, provide access to different employees and their permissions
Now admin will perform few actions
Harish(trainee) & Suresh(Devops engineer)

Creation of user.

Users should access only expense project cluster.

Role and Permissions

Resources -- Nouns
Verbs -- action 

Creation of EC2 - Verb 
1. What resources i should have access?

Data Center -- Noun 
Enter in to data center - Verb 
Creation of server in datacenter -- Verb, Create access
Update  existing server in datacenter -- Verb, update access 
Delete existing server in data center -- Verb, delete access



Admin will email project team that namespace and access is given.


1. Harish has access to EKS describe cluster r not
2. Get aws-auth configmap
3. User Harish authenticated
4. Check role and rolebindings


HPA(Horizontal Pod Autoscaling)
---------------------
Horizontal scaling
Create  one more server when traffic is increased
Create  one more server when traffic is increased
Create  one more server when traffic is increased
Remove one server when traffic is decreased


Vertical Scaling --- Common resources increasing, less privacy, building may not withstand
Increase RAM and HD and number of CPU cores
Increase RAM and HD and number of CPU cores
Increase RAM and HD and number of CPU cores

Autoscaling and autoscaling policy and metric CPU Utilization

24hrs --8hrs(metric)

A Pod is over utilized r less utilized how to decide? Based on resource definition

1. Metric server(kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml)
2. Resource definition inside Pod.
3. HPA should be attached to deployment

-------------------------
9th July Session 62

Taints and tolerations
Affinity and anti affinity
Ingress controller

Selectors
----
Master -- nodes
Scheduler --> Decide which node your pod should run

Node selector -- based on node labels you can decide which node your pod should run

1...us-east-1a
2...us-east-1b
3...us-east-1c

EBS volume is in 1a AZ. Using node selectors you can execute pod in node-1

taint == paint
If you apply some paint on 10rs note we cant exchange it. But you can exchange at RBI r banks.

If i paint k8 nodes, scheduler by default will not schedule pods in that node..

kubectl taint nodes node1 key1=value1:NoSchedule

NoSchedule ==  no new pods, existing pods will run

kubectl taint nodes node1 key1=value1:NoExecute

NoExecute == no new pods, existing pods will be evicted

PreferNoSchedule = try not to schedule, but cannot gaurantee.

Expense project have seperate node group with their custom configuration, so expense nodes are tainted by default.
Only pods related to expense will have tolerations.

Imagepullpolicy
-------------
Image is available in docker hub 

nodes will pull the image ---> image is present in the node

You change the image and push to hub..

imagepullpolicy --

ifNotPresent --> means pull the image, if it is laready not pulled inside the node.
Always ---> Pull it even it is present r not

Affinity
-----------
affinity = like

1. Scheduler --- Scheduler schedules the pod
2. Execution --- Pod should run

PreferredDuringSchedulingIgnoredDuringExecution--- soft
requiredDuringSchedulingIgnoredDuringExecution--- hard


requiredDuringSchedulingIgnoredDuringExecution --> Pod will not run even you set affinity. We need to apply tolerations.
So in this case until you apply toleration pod will be in pending state.

We have tainted a particular node, how can you run pod in that particular node?
We need to apply tolerations.

PreferredDuringSchedulingIgnoredDuringExecution --> Scheduler tries the asked node, but it cannot schedule because of tainted node.
Since it is only preferred it can schedule on to different node.


Pod affinity
-----------------
1 pod loves another pod

pod-1 is in node-1
pod-2 tries to be in node-1

Pod affinity and pod anti affinity
----------------

Application --> cache --> DB
Web store application should have 2 replicas
1 pod 1 cache
2nd pod and 2nd cache

1st cache pod
----------
2 replicas are requiredDuringSchedulingIgnoredDuringExecution
1st replica --> node -1
2nd replica --> node-2


2nd app pod
-------
2 replicas are requiredDuringSchedulingIgnoredDuringExecution
1st replica ---- node -1
2nd replica --- node -2

1st pod --- goes to any node, but as per affinity it should goto where redis is running
2nd pod --> goes to another node where pod-1 is not running, but it should go another pod where redis is already running

--------------------
10th July Session 63

Ingress Controller
--------
ALB -- rules, listeners, TGs..
Classic --> Old generation, AWS recommends to use ALB not classic LB.


ALB
--------
Intelligent load balancer
Create multiple listeners, create multiple rules

backend.alb-url--> backend target groups
user.alb-url --> user target group
m.facebook.com --> this is redirecting to mobile based website.
facebook.com--> redirected to desktop based site.

host based routing and
context based routing

sudheer459.online

hostbased
-----------
backend.sudheer459.online
user.sudheer459.online

context based r path based
---------
sudheer459.online/backend - Backend TG
sudheer459.online/user - user TG

https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/deploy/installation/

1. Create OIDC(OpenID Connect)
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8.
eksctl utils associate-iam-oidc-provider \
    --region <region-code> \
    --cluster <your-cluster-name> \
    --approve

2. curl -o iam-policy.json

aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam-policy.json

eksctl create iamserviceaccount \
--cluster=<cluster-name> \
--namespace=kube-system \
--name=aws-load-balancer-controller \
--attach-policy-arn=arn:aws:iam::<AWS_ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
--override-existing-serviceaccounts \
--region <region-code> \
--approve


Helm advactage
-----
1. Templatize the manifest files
2. package manager

yum --> install packages in rhel os

entire containeraization 2 approaches
building image and running image

Opensource images are always available in internet.
	jenkins is statfull
	
	statefulset(jenkins need to install in K8s)
	Service LB(jenkins need to expose in K8s)
	Configmap(jenkins config to need to provide Dynamic)
	Secret(Jenkins passowrd)
	pv
	pvc
	sc

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=expense-1 
--set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host
spec:
  rules:
  - host: "backend.daws78s.online"
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: backend
            port:
              number: 80


ALB --> Listener --> rule --> TG

ALB/Ingress Controller --> Ingress --> Service --> Pod


2 Applications -> appl1 and app2

app1.sudheer459.online --> response should come from APP1
app2.sudheer459.online --> response should come from APP2

app1.sudheer459.online --> route53-- that should point to ALB

HDFC -- ingresscontroller
IDFC --

---------------------------
11th July Session 64

Init Containers
ephemeral Volumes
	emptyDir
	hostpath
Liveness probe
readiness probe

Init Containers
---------------







