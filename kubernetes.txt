26th June Session 55

1. Build the image
2. Run the image

Problems(in docker)
---------------
1 pg --- 10 rooms

If power cut, water cut

10PGs -- 1pg water cut
He will shift persons to other PG temporary..


1 Host --- 3 containers if host crash..


Scalability
----------------
Normal servers with route53
Traffic increases, multiple servers and keep load balanccers and update R53 as LB URL.
If container crash, docker is not able to replace automatically. --> not reliable --> self healing
No proper volume management.
NO strong network connectivity.
No auto scaling 
No secrets and config management

3 Hosts -- 1 pod is  host-1 another pod is in host-2


////////////Benefits of using kubernetes and docker///
Scalability, High avaibility, portability, security, ease of use, reduce costs, improve agility, increase innovation
------------------------------------

Docker Architecture?
client = docker command

docker daemon = docker engine.... systemctl start docker..

daemon/server responds to client

docker run nginx --> checks image available in local r not, if not available pull it and keep it in local.
Run a container and send output to client.

Docker Architecture::How your system is getting request, how is it processing inside and 
how you r sending request back to the client.


Docker Architecture consists of docker client which is docker command line tool and
docker engine is nothing but docker host and the local registry and central registry.
Whenever you run a command in docker host it will connect to the daemon, it will check for the image local 
if it is not available it will pull from the central repository and keep it in local create a container of it and send 
a response to the client.

Docker = suit = image build + image pull through its hub + running the image as containers.
Docker is providing run time environmnent to run docker images.

Kubernates is a container orchestrator....

If you want to run 100 pg, we will take help of manager.. he needs some software.. 


Amazon master under this we have ec2s(nodes).
Request come to master.
Master will decide where to go

Nodes should run containers.
containers run time environment.

Here VS code will develop docker files and push to github.

kubectl - to connect kubernetes

docker run we have to give inputs --- image, ports, versions, volumes, name, environment variables.



eksctl --> create and manage EKS cluster
kubectl --> to manage containers in kubernetes cluster


kubectl get nodes --> display nodes in k8 cluster

command eksctl create cluster --config-file=<filename>

K8 resources
---------
Namespace --> Isolated project where you can create resources related to your project.

Basic syntax

kind:
apiVersion:
metadata:

spec:


kubectl apply -f <filename.yaml>
kubectl delete -f <filename.yaml>

pod is smallest deployable(runnable) unit in kubernetes



pod(EKS) vs container(Docker)
------------
Pod contains multiple containers. Container inside pod share same n/w and storage.

-----------------

27th June Session 56

1.Image Building
	Develop in VS Code
	Push to github
	pull in workstation
	build images
	push to docker hub
	
2. Login to workstation	
	Install docker, kubectl and eksctl
	Either aws configure or attach the role
	eks.yaml
	Now create cluster through eksctl command
	kubectl get nodes

3. Develop K8 YAML files
	Push to github
	Pull in workstation
	Apply using kubectl command
	
	
labels vs annotations

1 cpu = 1000m or 1024m


resources:
	  # soft limits
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m
	
If there is a code change, should i go for build r not? 
Ans: yes build

Code and configuration

What is configuration?
DB urls, DB usernames, passwords, api endpoint, other system urls.

Keep the key inside code, but value should be out of the code.

If changes, change the value and restart the application.

How can you access pod outside?
Ans: Services

Pods are ephemeral, IP address is temporary, but you can attach this to the service.

Service: 
	Acts as here loadbalancer
	service mesh
	
Cluster IP < Node Port < Load Balancer

1.Cluster IP
2. Node Port
3. Load Balancer

Labels are acted as selectors
Internet::: Other than kubernetes cluster everthing is called internet..

Nodeport --->
Internet--- external
Intranet --- internal(inside)
-------------------
28th June Session 57

Sets
---------
1. Replicaset -- Create replicas of pod.
2. Deployment
3. Daemonset
4. Statefulset


Pod is a subset of replica.

Pod = replicaset-name-<some-random-string>

Deployment: Removing old code, releasing new version of code, changes the application version.

Stop the server
remove old code
Download new code
Restart the server.

Deployment
--------------
Replicaset = deployment-name-<random-string>

pod = RS-<random-string>


nginx-68fcf7b7bf-b9qmf

nginx-8494fb8c88-5cbgf 



Deployment >> Replicaset >> Pod >> containers


https://github.com/ahmetb/kubectx

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


For security reasons we cant claim port no 80 for nginx, we can claim port no 8080

////For best practice if we seperate configuration code from application code we can reduce steps(new builds, new deploys)////
I dont want to rebuild the image bcz configuration is not part of the code and not best practice also

We can keep nginx configuration in a configmap
---------------------------
1st July Session 58

Volumes in Kubernates
------------------
1. External HD
	SSD, Pendrive, etc.,
	
2. Google/microsoft drive

EBS/EFS
-------------
1. HD should be as near as possible to the computer.
2. Google drive, can be anywhere in the world. Network is used for file sharing. NFS(network file sharing)

Static and Dynamic provisioning

Static
------------
EBS

Mumbai-- Region

North Mumbai --AZ-1a
South Mumbai --AZ-1b

1. I need to create EBS volume
2. You need to install drivers. I have to install EBS related drivers in EKS cluster.
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
3. Your nodes should have access to connect with EBS volumes. Attach EBSCSI policy to the EC2 instance role.

wrappers..

You can represent EBS volume inside kubernetes with a resource called Persistant Volume. This is equivalent to EBS storage.


PVC
----
Persistant volume claim. Pods should claim i want the volume. Pods can claim through resource called PVC.

1. If you PV is not available
---------
check drivers installed r not
check your EC2 have tje role and permissions

If instances are created in 1A, 1B, 1C and Volume is in 1C. in this case we have to use node selectors.

nodeSelector:
    topology.kubernetes.io/zone: us-east-1c
	
and this is called as static provisioning

1. Admins
2. Users

As a Application devops engineer, i will get access to expense namespace

1. Users send email to storage team for creation of volume. AWS Storage team will create volume and give you ID.
2. Users send email to K8s admin to create PV for them the ID.
3. Users can claim using PVC.

Dynamic Provisioning
-------------
1. You need to install drivers. I have to install EBS related drivers in EKS cluster.
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
2. Your nodes should have access to connect with EBS volumes. Attach EBSCSI policy to the EC2 instance role.
3. Someone on behalf of you should create volume and equivalent PV automatically. THis is called Dynamic provisioning that
one is storage class


So admins create storage class for every project as best practice.

Static---------
Kid -- Mother -- Fater --- MOney
Pod -- PVC -- PV --- Storage/Volume


Dynamic
------------
Kid -- Mother -- E-wallet -- Money
Pod -- PVVC -- Storage class -- Storage


EFS -- 
--------
EBS vs EFS

1. EBS is like hard disk, volume and instance should be in same AZ-1a
2. EFS is through NFS protocol, it can be anywhere
3. EBS volume is static, it can increase on its own.
4. EFS is completely dynamic in storage, it can automatically expand memory.
5. Which is suitable for DB applications like MYSQL
--------------------------------------
3th July Session 58

Cluster creation with eksctl
namespaces
Pods (multicontainer, environment,labels,resources,annotations)
configmap
secrets
Services
	ClusterIP
	nodeport
	Loadbalancer
Sets
	Replicaset
	Deployment
expense using deployment
	How to use configmap as volume
Staic provisioning
	EBS
Dynamic Provisioning
	EBS
EBS vs EFS

EBS
----------
1. Create Volume
2. Install EBSCSI drivers
3. EC2 should have role to access EBS
4. Create PV equivalent to EBS Volume --> admin, CLuster level
5. Create PVC --> namespace level user can do this
6. Use PVC in pod


Dynamic
------------
SC --> admin level, this creates volume and PV automatically.
PVC, mention SC name


EFS
------------
1. Create EFS volume
2. Allow EC2 instances on NFS protocol in EFS SG
3. install EFS driveresx`

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.0" > private-ecr-driver.yaml

Apply the manifest..
kubectl apply -f private-ecr-driver.yaml


4. Check IAM access to the instances
AmazonEFSCSIDriverPolicy


LB --> Node on particular port--> Nodes should allow traffic from LB --> nodeport --> cluster IP --> pod

EFS Dynamic
---------
1. Installing drivers

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.0" > public-ecr-driver.yaml

Apply the manifest..
kubectl apply -f private-ecr-driver.yaml

2. Check EC2 instance role have access or not
3. Check EFS Volume.
4. Check EFS SG allowing EC2 SG
5. Create SC

MySQL
-----------
EBS

1. Install EBS CSI drivers
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"
2. Check ebs csi permissions to node
AmazonEBSCSIDriverPolicy

3. create expense storage class, namespace

Inform application team that SC and namespace created. Drivers installed permissions given to nodes..

StatefulSet -->>> Used for stateful applications i.e., which should have some storage.
Deployment --> Used for stateless applications i.e., usually frontend and backend

Pods are created with -0, -1 in statefulset, because statefulset should keep the pod identity.
Pods in deployment are created at a time, but pods in statefulset will be created in orderly manner.

Stateful set must have headless service. 

A service which will not have cluster IP is called Headless

While dealing database cluster, one node should be aware of other nodes.

-------------------------------
4th July Session 60

PV -- Representing physical storage
PVC --Claiming the storage
SC -- Automatic creation of volume and PV based on the claim from PVC

Deployment vs Statefulset
--------------------
1. Stateful application like DB
2. PV and PVC are mandatory for statefulset.
3. Orderly provisioning of pods happens.
4. Pods keep their identity like name
5. We must create headless service.
6. Every pod should have its own storage. So PV and PVC


1. Install EBS drivers
2. Check EC2 role permissions
3. Create storage class
4. Create statefulset

'''''''''''''''''''' Statefulset
If deployment, nslookup of service gives, service IP address
If statefulset,nslookup of headless service gives us IP address of all pods.
''''''''''''''''''''
Headless service is for Cluster features
Normal service is for to connect external users.


HELM
------------
1. Building image
2. Running image

Only few values frequently change in manifest files.

1. Kubernetes package manager
2. templatise manifest files

HELM install commands...
$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh
......

1. Chart.yaml

Install helm chart

helm install <preferred-name>. --> . represents Chart.yaml in current directory

helm uninstall nginx
help history nginx
helm rollback nginx
helm list
helm upgrade nginx
helm status nginx

-----------------------
5th July Session 61

RBAC(Role Based access control)
--------------
expense

trainee - read access
Sr engineer -- deployments access
TL -- namespace admin access
Manager -- cluster admin(all access)

Authentication and Authorization


Role
RoleBinding -- user ni roleki bind cheyadam
ClusterRole
ClusterRoleBinding

AWS IAM -- RBAC for AWS IAM
EKS --PaaS, AWS EKS can use IAM as authentication provider. SSO

1. TL will send an email to create namespace, provide access to different employees and their permissions
Now admin will perform few actions
Harish(trainee) & Suresh(Devops engineer)

Creation of user.

Users should access only expense project cluster.

Role and Permissions

Resources -- Nouns
Verbs -- action 

Creation of EC2 - Verb 
1. What resources i should have access?

Data Center -- Noun 
Enter in to data center - Verb 
Creation of server in datacenter -- Verb, Create access
Update  existing server in datacenter -- Verb, update access 
Delete existing server in data center -- Verb, delete access



Admin will email project team that namespace and access is given.


1. Harish has access to EKS describe cluster r not
2. Get aws-auth configmap
3. User Harish authenticated
4. Check role and rolebindings


HPA(Horizontal Pod Autoscaling)
---------------------
Horizontal scaling
Create  one more server when traffic is increased
Create  one more server when traffic is increased
Create  one more server when traffic is increased
Remove one server when traffic is decreased


Vertical Scaling --- Common resources increasing, less privacy, building may not withstand
Increase RAM and HD and number of CPU cores
Increase RAM and HD and number of CPU cores
Increase RAM and HD and number of CPU cores

Autoscaling and autoscaling policy and metric CPU Utilization

24hrs --8hrs(metric)

A Pod is over utilized r less utilized how to decide? Based on resource definition

1. Metric server(kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml)
2. Resource definition inside Pod.
3. HPA should be attached to deployment

-------------------------
9th July Session 62

Taints and tolerations
Affinity and anti affinity
Ingress controller

Selectors
----
Master -- nodes
Scheduler --> Decide which node your pod should run

Node selector -- based on node labels you can decide which node your pod should run

1...us-east-1a
2...us-east-1b
3...us-east-1c

EBS volume is in 1a AZ. Using node selectors you can execute pod in node-1

taint == paint
If you apply some paint on 10rs note we cant exchange it. But you can exchange at RBI r banks.

If i paint k8 nodes, scheduler by default will not schedule pods in that node..

kubectl taint nodes node1 key1=value1:NoSchedule

NoSchedule ==  no new pods, existing pods will run

kubectl taint nodes node1 key1=value1:NoExecute

NoExecute == no new pods, existing pods will be evicted

PreferNoSchedule = try not to schedule, but cannot gaurantee.

Expense project have seperate node group with their custom configuration, so expense nodes are tainted by default.
Only pods related to expense will have tolerations.

Imagepullpolicy
-------------
Image is available in docker hub 

nodes will pull the image ---> image is present in the node

You change the image and push to hub..

imagepullpolicy --

ifNotPresent --> means pull the image, if it is laready not pulled inside the node.
Always ---> Pull it even it is present r not

Affinity
-----------
affinity = like

1. Scheduler --- Scheduler schedules the pod
2. Execution --- Pod should run

PreferredDuringSchedulingIgnoredDuringExecution--- soft
requiredDuringSchedulingIgnoredDuringExecution--- hard


requiredDuringSchedulingIgnoredDuringExecution --> Pod will not run even you set affinity. We need to apply tolerations.
So in this case until you apply toleration pod will be in pending state.

We have tainted a particular node, how can you run pod in that particular node?
We need to apply tolerations.

PreferredDuringSchedulingIgnoredDuringExecution --> Scheduler tries the asked node, but it cannot schedule because of tainted node.
Since it is only preferred it can schedule on to different node.


Pod affinity
-----------------
1 pod loves another pod

pod-1 is in node-1
pod-2 tries to be in node-1

Pod affinity and pod anti affinity
----------------

Application --> cache --> DB
Web store application should have 2 replicas
1 pod 1 cache
2nd pod and 2nd cache

1st cache pod
----------
2 replicas are requiredDuringSchedulingIgnoredDuringExecution
1st replica --> node -1
2nd replica --> node-2


2nd app pod
-------
2 replicas are requiredDuringSchedulingIgnoredDuringExecution
1st replica ---- node -1
2nd replica --- node -2

1st pod --- goes to any node, but as per affinity it should goto where redis is running
2nd pod --> goes to another node where pod-1 is not running, but it should go another pod where redis is already running

--------------------
10th July Session 63

Ingress Controller
--------
ALB -- rules, listeners, TGs..
Classic --> Old generation, AWS recommends to use ALB not classic LB.


ALB
--------
Intelligent load balancer
Create multiple listeners, create multiple rules

backend.alb-url--> backend target groups
user.alb-url --> user target group
m.facebook.com --> this is redirecting to mobile based website.
facebook.com--> redirected to desktop based site.

host based routing and
context based routing

sudheer459.online

hostbased
-----------
backend.sudheer459.online
user.sudheer459.online

context based r path based
---------
sudheer459.online/backend - Backend TG
sudheer459.online/user - user TG

https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/deploy/installation/

1. Create OIDC(OpenID Connect)
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8.
eksctl utils associate-iam-oidc-provider \
    --region <region-code> \
    --cluster <your-cluster-name> \
    --approve

2. curl -o iam-policy.json

aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam-policy.json

eksctl create iamserviceaccount \
--cluster=<cluster-name> \
--namespace=kube-system \
--name=aws-load-balancer-controller \
--attach-policy-arn=arn:aws:iam::<AWS_ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
--override-existing-serviceaccounts \
--region <region-code> \
--approve


Helm advactage
-----
1. Templatize the manifest files
2. package manager

yum --> install packages in rhel os

entire containeraization 2 approaches
building image and running image

Opensource images are always available in internet.
	jenkins is statfull
	
	statefulset(jenkins need to install in K8s)
	Service LB(jenkins need to expose in K8s)
	Configmap(jenkins config to need to provide Dynamic)
	Secret(Jenkins passowrd)
	pv
	pvc
	sc

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=expense-1 
--set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host
spec:
  rules:
  - host: "backend.daws78s.online"
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: backend
            port:
              number: 80


ALB --> Listener --> rule --> TG

ALB/Ingress Controller --> Ingress --> Service --> Pod


2 Applications -> appl1 and app2

app1.sudheer459.online --> response should come from APP1
app2.sudheer459.online --> response should come from APP2

app1.sudheer459.online --> route53-- that should point to ALB

HDFC -- ingresscontroller
IDFC --

---------------------------
11th July Session 64

Init Containers
ephemeral Volumes
	emptyDir
	hostpath
Liveness probe
readiness probe

Init Containers
---------------
1. Init Containers run before the main containers run. It can be one r many
2. Init containers should be completed one by one
3. If init containers fail main containers will not run.
4. To wait until other applications are ready, to prepare some configurations for main container.

DB and backend both are started at a time. Backend is ready before DB started successfully. Backend is getting
failed bcz no connection to DB.

We can have init container for backend. So before backend starts init container should be checkinf if DB is running r not.

To check the DB connection.... We have DB and We have the backend application before starting the backend we have an
init container that will check the connection to the DB. The connection is success then only we will start the backend
container, otherwise we wont start..


Containers best practices
---
1. Without root user
2. use volumes
3. Use official images
4. Use samll images, dont keep unnecessary installations
5. use multi stage builds
6. use health checks
7. use custom network
8. reduce image layers


readiness probe(when it will get ready)
-----------------
We can define when our container is ready.
For backend we can say when port 8080 opened, we can say it is ready.


Liveness probe(health check)
------------
Health check we can do once container is ready.
Auto healing 


liveness probe will happen frequently, whenever it finds something is wrong it will auto restart the container.

ephemeral volumes
	emptyDir
	hostPath

static provisioning and dynamic provisioning --> perm volumes

ephemeral volumes
--------------------
1. pod is running and generating log. does it generate log after terminated?
No it will not generate.

/var/log/nginx/access.log
/var/log/nginx/error.log

ephemeral volumes --> emptyDir

1. nginx --> main container
2. filebeat --> sidecar

you should somehow mount the volume/storage to both containers.

by default k8 considers volumes as emptyDir.


docker run -d -v /:/spam nginx


Daemon set
------------------
Daemonset will make sure a pod runs in every node. why this is useful?

we have to access underlying node logs and push them to elastic search for log monitoring. if we run deamonset in kubernetes, it make sure a pod is automatically created when new node is added.

zeal vora --> AWS security specialist

-----------------
12th July Session 65

Cluster creation and upgrade
-------------
Admin Activity

aws eks update-kubeconfig --region region-code(us-east-1) --name my-cluster(expense-dev)

Cluster is running, few application are running inside

Cluster upgrade is planned
----------------------
No downtime to the existing applications but no new release and no new deployments in the upgrade time.

Communications sent about cluster upgrade activity is started.

1. First we need to get another node group called green.
2. Taint the green nodes, so that they should not get any pods scheduled.

3. Now upgrade your control plane

If any issues in control plane/EKS then u r able to access your application r not?
There is no relation application and EKS if any issue also we can access 
BCZ LB always connected to nodegroup
Unless nodes are down no issue for application

4. upgrade green node group also to 1.30
5. Shift workloads from 1.29 node group to 1.30 node group
6. Taint blue nodes(cordon blue nodes)
7. Untaint green nodes
8. Drain blue nodes
Inform all stake holders, applications teams. Perform sanity checks/testing
close the acivity.

--------------------------------
15th July Session 66

INFRAAAAA

1. Existing ALB attach to EKS(suggest)
2. Ingress creating ALB

ALB is never part of EKS cluster, easy to create r part of infra if created outside of EKS.
YOu can create listerners, attach SSL certificates, create rules, target groups et.,

You can easily attach pods directly to ALB TG using TG binding resource.


//////After getting Docker/Kubernetes we forget VM deployments so Ansible got fade out.///////////

But Ansible will use in EKS nodes to develop AMIs... like linux admin, EKS nodes to customize

aws eks update-kubeconfig --region us-east-1 --name expense-dev

EKS nodes should accept traffic from nodes with in our VPC CIDR range

Let
node-1 : 10.0.11.23
node-2: 10.0.22.29

Database
------------
Created DB and schema.. DB admins should create necessary tables and users for applications..

Now if u want to connect DB then we need access from bastion to RDS.
for that we need to give SGs port 3306


mysql -h db-dev.sudheer459.online -u root -pExpenseApp1
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 18
Server version: 8.0.35 Source distribution

Copyright (c) 2000, 2024, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> USE transactions;
Database changed
mysql> CREATE TABLE IF NOT EXISTS transactions (
    ->     id INT AUTO_INCREMENT PRIMARY KEY,
    ->     amount INT,
    ->     description VARCHAR(255)
    -> );
Query OK, 0 rows affected (0.03 sec)

mysql> CREATE USER IF NOT EXISTS 'expense'@'%' IDENTIFIED BY 'ExpenseApp@1';
Query OK, 0 rows affected (0.00 sec)

mysql> GRANT ALL ON transactions.* TO 'expense'@'%';
Query OK, 0 rows affected (0.00 sec)

mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)

mysql> exit

If any application is in K8s we have togo through ingress controller only.

The main difference is ingresses are native objects inside the cluster that can route to multiple services, 
while load balancers are external to the cluster and only route to a single service



One more application want togo internet
--------------------
1. Create TG
2. Create rule
3. Check route53

One EKS cluster can host multiple applications
R53Roboshop.sudheer459.online ---> pointing to our ingress ALB
rule roboshop.sudheer459.online ---> pointing to roboshop TG


EKS Ingress Setup
--------------------
eksctl utils associate-iam-oidc-provider \
    --region us-east-1 \
    --cluster expense-dev \
    --approve
	
curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.1.2/docs/install/iam_policy.json

aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam-policy.json


eksctl create iamserviceaccount \
--cluster=expense-dev \
--namespace=kube-system \
--name=aws-load-balancer-controller \
--attach-policy-arn=arn:aws:iam::851725359401:policy/AWSLoadBalancerControllerIAMPolicy \
--override-existing-serviceaccounts \
--approve

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system
--set clustername=<cluster-name>

------------------------
16th July Session 67

K8s Architecture

Master
Node

API Server -- It is the main component responding to all user requests. It runs on port 443

Scheduler --> responsible to schedule the pods on to nodes. it checks multiple things while scheduling.
 taints and tolerations, node selectors, affinity, underlying server memory, their cpu consumption etc.



Controll manager
--------------
replica controller component --> replicaset --> if you delete the pods, it will automatically create new pod

node controller --> checks the nodes all the time, whether they are ready, able to schedule r not.
 if a node is down it will make sure another node is created and shift the pods to diff nodes

ServiceAccount controller --> creates service account for every namespace created. basically under internal authentication

etcd
-----------
entire k8 cluster data is in etcd. it is very important to take frequent backup. if cloud, they are responsible for this




Node components
-----------------
kube-proxy --> on behalf of someone. it intercepts every incoming and outgoing request of node.
 kube-proxy maintains network rules on nodes. these network rules allow network communication to your Pods

kubelet --> it connects nodes to control plane. An agent that runs on each node in the cluster.
 it pulls the information from control plane and runs the pods.

container run time --> it is on every node, it basically runs the images into containers. containerd is the runtime usually.

Add-ons
-----------
kube-dns --> provides DNS to the pods. http://backend:8080. 
kubedns provides IP address to the service when one pod wants to connect with other pod.

networking plugins --> VPC CNI, default network inside EKS.


aws eks update-kubeconfig --region us-east-1 --name expense-dev

DB
------
check the schema created or not
check the tables created or not to store the data
check the users created or not

create namespace

kubectl create namespace expense

configure EKS ingress controller

Dockerfile --> build and push the images
manifest --> how to run the image


aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 851725359401.dkr.ecr.us-east-1.amazonaws.com


docker build -t 851725359401.dkr.ecr.us-east-1.amazonaws.com/expense-backend:latest .


docker push 851725359401.dkr.ecr.us-east-1.amazonaws.com/expense-backend:latest

Below Errors..

imagePullBackOff --> your k8 node is unable to pull the image
(Version is wrong,repository name r address,Connection cut issue/network. We have to check image address)

crashLoopBackoff --> pod is not able to run... if Backend is not able to connect DB
(crash... we need to check logs)

pending --> PV and PVC setup and config are not proper. pod is not able to mount the volume.
(We have to describe and check logs)

////////////
In VMs deployment is huge process. While in Docker/Kubernetes its simple process.
Only Image build and manifest push.
//////////////

Shared levels

HDFC Bank
-----------
10 accounts
	Credit card
	savings account
	stocks
	insurance
	home loans
	personal loans
	business loans
	
	
200 components

Backend --- java, python...
frontend-- angular, react


DRY -- Dont repeat yourself, reuse. Centralise everything
Ansible Roles
Terraform module

If you dont centralise/reuse your pipeline, you need to manage them manually each and everyone.
If there is a change you need to update in everything jenkinsfile pratically very tough.

Different persons can write different pipelines, you cant enforce best standards.

Jenkins shared library --- sharing pipelines as a library

We are using jenkins shared libraries and inform all teams so that they will call r...
---------------

Developer..

fetch DB records asap
	transaction management
	
	use less memory
	increase the speed

Devops
-----------
Automate manual tasks



Conflict with Devops and Developeer 

Config miss(credentials)
Devops responsibility--- Connections are working properly r not. Request went to frontend r not

Configuration issue Devops problem
Code issue Developer problem
-----------------------------
17th July Session 68








