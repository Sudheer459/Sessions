Playbook - Keep all your modules in a single file with YAML syntax.

A Terraform module is a collection of standard configuration files in a dedicated directory. 

A Kubernetes manifest file is a YAML or JSON file that describes the desired state of a Kubernetes object in a cluster.


Pipeline Syntax

Pipeline{
	agent{
		label 'java-agent'
	}
	options{
		timeout(time:30, unit: 'MINUTES')
	}
	Environment{
		Greeting = 'Hello World'
	}
	stages{
		stage('Build'){
			steps{
				sh 'echo this is build'
			}
		
		}
		post{
		 always{
		 }
		 success{
		 }
		 failure{
		 }
	}
}

----------------
kind: Pod
apiVersion: v1
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:stable-otel
-----------------------

Using Jenkins and its shared libraries you have ensured that your application code is bug-free and secure too.
Later on you build the image and push the image to a private ECR using Docker.
Then using terraform you have created the necessary infrastructure such as a VPC, SGs, EKS, ALB, RDS, ACM, 
Workstation etc for your application deployment.
At last you have deployed the application to the EKS cluster.
Hope this gives you a better understanding of how to integrate all the tools together.
----------------
Devops pipeline stages... PCBTRMO
Plan, code, build, test, release, monitor, operate
--------------------
Devops pipeline process..
SCM
Build Automation
Automated Testing
Deployment Automation
---------------------------
Here are all the tools commonly used in DevOps across various industries:

1)Cloud Computing: AWS ☁️
2)Version Control: Git 📁
3)Pipelines: Jenkins 🚀
4)Configuration Management: Ansible 🔩
5)Containerization: Docker 🐳
6)Container Orchestration: Kubernetes (K8S) ☸️
7)Monitoring and Logging: Prometheus 📊
8)Infrastructure as Code: Terraform 🌐

These tools help streamline the DevOps process, making software development, deployment, and 
management more efficient and effective.
-------------
Prometheus

Cloud Environment: Prometheus should find the nodes automatically and add them to scrape list

Service Discovery:
---------------------
 Underlying nodes should have node exporter installed and started

Describe ec2 instances ---> listing ec2 instances---> get the IP address

filter using tags, Monitoring -- true 

if value < 1, then raise the alert

1. raise the alert
2. manage the alert

2 types of metrics
counter and Gauge

Latency
errors
traffic
saturation --> Prometheus

ELK -- Elastic kibana logstash
----------------
Elasticsearch --> repository,which stores something.. logs storage..DB application

KIbana ---> Visualisation/UI tool for elasticsearch 

Logstash: Filtering the logs before storing to elasticsearch.

Scheduler --- Scheduler schedules the pod

Daemon set :: Daemonset will make sure a pod runs in every node.

etcd: entire k8 cluster data is in etcd.

kube-proxy --> on behalf of someone. it intercepts every incoming and outgoing request of node.
 kube-proxy maintains network rules on nodes. these network rules allow network communication to your Pods

kubelet --> it connects nodes to control plane. An agent that runs on each node in the cluster.
 it pulls the information from control plane and runs the pods.
 
 -------------
 Below Errors..

imagePullBackOff --> your k8s node is unable to pull the image
(Version is wrong,repository name r address,Connection cut issue/network. We have to check image address)

crashLoopBackoff --> pod is not able to run... if Backend is not able to connect DB
(crash... we need to check logs)

pending --> PV and PVC setup and config are not proper. pod is not able to mount the volume.
(We have to describe and check logs)
 -----------------------------
 
 

Containers best practices
---
1. Without root user
2. use volumes
3. Use official images
4. Use small images, dont keep unnecessary installations
5. use multi stage builds
6. use health checks
7. use custom network
8. reduce image layers

----------------------
Cluster upgrade is planned
----------------------
No downtime to the existing applications but no new release and no new deployments in the upgrade time.

Communications sent about cluster upgrade activity is started.

1. First we need to get another node group called green.
2. Taint the green nodes, so that they should not get any pods scheduled.

3. Now upgrade your control plane

If any issues in control plane/EKS then you are able to access your application or not?
There is no relation b/w application and EKS if any issue also we can access 
because LB always connected to nodegroup. Unless nodes are down no issue for application

4. upgrade green node group also to 1.30
5. Shift workloads from 1.29 node group to 1.30 node group
6. Taint blue nodes(cordon blue nodes)
7. Untaint green nodes
8. Drain blue nodes

Inform all stake holders, applications teams. Perform sanity checks/testing(Application team will do)
close the acivity.
-------------------
---------------------AWS------------

We have migrated one of kuwait operator application name ooredoo from Onprem to Cloud. 
As it is a Internet based application, most of the traffic comes either from Internet users 
or from corporate users. So we have designed a Landing Zone Architecture where,
two AWS Accounts are commonly used by all the applications like Web, Data Analytical applications. 
These two AWS accounts are DMZ, Central Management.
 In DMZ, we installed Palo Alto(check point) firewalls in HA state and 
 this account is our single point of entry/exit for any traffic which goes to other accounts. 
 In Central Management Account where all our security controls like 
 (Bastion - Jump Server, TrendMicro - EndPoint Security, Qualys Scanner - Vulnerability scan) 
 are provisioned. On top of these 2 AWS accounts, Application specific RTL accounts
  for Web Application and Data Analytical Applications are plugged in to TGW which is created in Central Mgmt.

 


Traffic Flow : When Internet Users tries browse ooredoo.com.kw the request will be redirected 
to corporate (route53)DNS server where we whitelisted 2 EIPs against the website name. 
Further from EIPs traffic will be redirected to NLB IPs and then the instances(WebServer)
 attached to the Target Group. Most of the information is cached in Webservers and the 
 information that Users requested will be served from WebServers1 or WebServer2 as per 
 the Load Balancer requests.

///We have eth 0123
Eth2 is untrustable  due to public network //

 
We also have a DR setup in Ohio region, where we have the setup of Production 
Application Servers (WebServer, Publisher, Author, RDS, AD). 
Everyday Production Application Servers Volumee snapshots are copied to Ohio Region as 
per the Backup Policy Scheduled. So when the main website is down, using the latest snapshots, 
we create volumes and attach them to DR Application servers and bring 
the application back with minimal downtime

---------------------DevOps--------

Using Jenkins and its shared libraries you have ensured that your application code is bug-free and secure too.
Later on you build the image and push the image to a private ECR using Docker.
Then using terraform you have created the necessary infrastructure such as a VPC, SGs, EKS, ALB, RDS, ACM, 
Workstation etc for your application deployment.
At last you have deployed the application to the EKS cluster.
Hope this gives you a better understanding of how to integrate all the tools together.
//////////////////////////////////////////////////////////



How do you configure DevOps to a project?

1. Setup
2. Operations
3. Improvements

Developers are DevOps customers. We let them only to do best coding. 
apart from that we will take care of everything from DEV to PROD.

Our goals

1. fast releases
2. less defects
3. DEV to PROD automated deployments
                                        
as a devops we always look for best practices in the market. Our process should continously evolve. 
Best practices we are following now are

1. Shift left --> shift everything to DEV environment. Build, scan, deploy and test
2. DevSecOps --> Static source code analysis, SAST, DAST, OSS, Image and container scanning
3. Build once and run anywhere --> build artifact/image in only DEV environment, 
promote to other environments.
4. DRY --> dont repeat yourself

We should have a meeting with developers

1. which programming language and versions
2. what is the deployment platform
3. Discuss about branching and merging strategy. prefer feature branching for agile
4. if it is legacy then prefer git model

SRE --> Site reliability engineering
Git --> repos creation, settings, PR process, approvals, etc.
Jenkins --> Folder creation for their project, RBAC
Sonar --> project creation, setup quality gates, prefer zero tolerance. no vulnerabilties,
 no code snells, security rating A, min 80% code coverage
Nexus --> create a repo and policy
AWS --> R53 records, VPC if new project, IAM roles and permissions, ECR repos, SG
EKS --> create namespace in all environments, RBAC, etc. 
We already implemented ingress controller, static and dynamic provisioning, etc.
Terraform --> create S3 and dynamodb. automate infra creation through modules

We can explain about process, pipelines. We are using shared pipelines powered jenkins shared library. 
These are centralised managed by us. We have different combinations based on programming language
 and deployment platform.

1. NodeJSVM, NodeJSEKS
2. JavaVM, JavaEKS
3. PythonVM, PythonEKS

if they have new platform we can provide the solution. 

we keep a simple jenkinsfile that can call our pipelines in run time. 
whenever a developer pushes new commit we trigger pipeline through webhook

1. development pipeline where we follow shift left
	clone
	build
	unit testing
	scan --> all scans
	artifact upload
	push image
	deploy
	functional test --> we configure as per QA team inputs. we can use selenium
2. once DEV is success, they raise PR, get the approvals, 
if it is approved they can move app from DEV to higher environments like QA, UAT, SIT, etc.

3. NON-PROD pipeline --> fetch the image, do the deployment, integration testing

4. app is ready for PROD

We have CR process. APP support team raise CR, add all stakeholders, mention the time, test results, scan, etc.

once CR is reviewed and approved. app support team triggers PROD pipeline

get the CR number, have one more check. is CR in approved state and trigger time
 is with in deployment window or not. fetch the image and deploy --> app team perform sanity testing


apart from this we own EKS. we use to upgrade using Blue/green approach.

Continously we check the industry practices and try to implement, after the microsoft outage. We reviewed all our vendors process. We checked our integration testing process again with our vendor releases.

When app team upgrades the language version, we check the pipelines once again.

Monitoring
------------
Log monitoring
Server monitoring
application monitoring
traces
--------------
Using Jenkins and its shared libraries you have ensured that your application code is bug-free and secure too.
Later on you build the image and push the image to a private ECR using Docker.
Then using terraform you have created the necessary infrastructure such as a VPC, SGs, EKS, ALB, RDS, ACM, Workstation etc for your application deployment.
At last you have deployed the application to the EKS cluster.
Hope this gives you a better understanding of how to integrate all the tools together.


